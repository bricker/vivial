---
apiVersion: networking.gke.io/v1
kind: ManagedCertificate
metadata:
  name: core-api-cert
  namespace: "${NAMESPACE}"
spec:
  domains:
    - "api.${EAVE_ROOT_DOMAIN}"

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: core-api-ingress
  namespace: "${NAMESPACE}"
  annotations:
    networking.gke.io/v1beta1.FrontendConfig: shared-fc
    networking.gke.io/managed-certificates: core-api-cert
    kubernetes.io/ingress.global-static-ip-name: api-dot-eave-dot-dev-addr
    kubernetes.io/ingress.class: gce
  labels:
    app: core-api-app
spec:
  # ingressClassName: gce # This does not work. Without the "ingress.class" annotation, the LB isn't created.

  # The NOOP service is meant to always fail. It prevents external traffic from accessing paths that aren't whitelisted here.
  # GKE provides a "default-http-backend" service that is used if defaultBackend isn't specified here.
  # However, the response that it returns is a 404 with a message that exposes details about the infrastructure, and is therefore unsuitable.
  # defaultBackend:
  #   service:
  #     name: noop
  #     port:
  #       number: 65535
  rules:
    - host: "api.${EAVE_ROOT_DOMAIN}"
      http:
        paths:
          # Supported public endpoint prefixes.
          # Everything else is only accessible from the cluster.
          # TODO: a better place to define these?
          - pathType: Prefix
            path: "/status"
            backend: &backend
              service:
                name: core-api
                port:
                  name: http

          - pathType: Prefix
            path: "/public"
            backend: *backend

          - pathType: Prefix
            path: "/oauth"
            backend: *backend

          - pathType: Prefix
            path: "/favicon.ico"
            backend: *backend

---
apiVersion: v1
kind: Service
metadata:
  name: core-api
  namespace: "${NAMESPACE}"
  annotations:
    beta.cloud.google.com/backend-config: '{"default": "shared-bc"}'
spec:
  selector:
    app: core-api-app
  type: NodePort
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: app

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ksa-app-core-api
  namespace: "${NAMESPACE}"
  annotations:
    iam.gke.io/gcp-service-account: "gsa-app-core-api@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: core-api-deployment
  namespace: "${NAMESPACE}"
spec:
  selector:
    matchLabels:
      app: core-api-app
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
  template:
    metadata:
      labels:
        app: core-api-app
    spec:
      serviceAccountName: ksa-app-core-api
      containers:
        - name: core-api
          image: "us-central1-docker.pkg.dev/${GOOGLE_CLOUD_PROJECT}/docker/core-api:${APP_VERSION}"
          resources:
            # requests and limits must always be the same on Autopilot clusters without bursting.
            # if requests is omitted, the limits values are used.
            limits:
              memory: "1Gi"
              cpu: "250m"

          envFrom:
            - secretRef:
                name: metabase-jwt-shared-secret
            - secretRef:
                name: core-api-secret
            - configMapRef:
                name: shared-configmap
            - configMapRef:
                name: core-api-configmap

          env:
            - name: EAVE_DB_USER
              value: "gsa-app-core-api@${GOOGLE_CLOUD_PROJECT}.iam"
            - name: EAVE_DB_HOST
              value: "127.0.0.1"
            - name: EAVE_DB_PORT
              value: "5432"
            # gunicorn options
            - name: GUNICORN_CMD_ARGS
              value: "--bind=0.0.0.0:8000 --workers=3 --timeout=90"

          ports:
            - name: app
              containerPort: 8000

          readinessProbe:
            httpGet:
              path: /status
              port: app

          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /status
              port: app

        - name: cloud-sql-proxy
          # This uses the latest version of the Cloud SQL Proxy
          # It is recommended to use a specific version for production environments.
          # See: https://github.com/GoogleCloudPlatform/cloudsql-proxy
          image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:latest
          args:
            # Enable healthcheck endpoints for kube probes
            - "--health-check"
            - "--http-address=0.0.0.0" # Bind to all interfaces so that the Kubernetes control plane can communicate with this process.
            - "--http-port=9090" # This is the default

            # If connecting from a VPC-native GKE cluster, you can use the
            # following flag to have the proxy connect over private IP
            - "--private-ip"

            # If you are not connecting with Automatic IAM, you can delete
            # the following flag.
            - "--auto-iam-authn"

            # tcp should be set to the port the proxy should listen on
            # and should match the DB_PORT value set above.
            # Defaults: MySQL: 3306, Postgres: 5432, SQLServer: 1433
            - "--port=5432"
            - "--structured-logs"
            # - "--unix-socket /cloudsql"
            - "${PG_CONNECTION_STRING}"
          # volumeMounts:
          #   - name: cloudsql-socket
          #     mountPath: /cloudsql
          securityContext:
            # The default Cloud SQL proxy image runs as the
            # "nonroot" user and group (uid: 65532) by default.
            runAsNonRoot: true

          resources:
            limits:
              cpu: 500m
              memory: 2Gi

          ports:
            - name: healthcheck
              containerPort: 9090
            - name: proxy
              containerPort: 5432

          # https://github.com/GoogleCloudPlatform/cloud-sql-proxy/tree/54e65d14a5d533f44e33b52a2dc88c2a419eae2f/examples/k8s-health-check#running-cloud-sql-proxy-with-health-checks-in-kubernetes

          startupProbe:
            httpGet:
              path: /startup
              port: healthcheck
            periodSeconds: 1
            timeoutSeconds: 5
            failureThreshold: 20

          # The documentation does not recommend using the readiness probe.
          # The cloud-sql-proxy readiness probe checks for issues that can usually resolve themselves, so this check could restart the container unnecessarily.
          # readinessProbe:
          #   httpGet:
          #     path: /readiness
          #     port: healthcheck
            # initialDelaySeconds: 30
            # # 30 sec period x 6 failures = 3 min until the pod is terminated
            # periodSeconds: 30
            # failureThreshold: 6
            # timeoutSeconds: 10
            # successThreshold: 1

          livenessProbe:
            httpGet:
              path: /liveness
              port: healthcheck
            # Number of seconds after the container has started before the first probe is scheduled. Defaults to 0.
            # Not necessary when the startup probe is in use.
            initialDelaySeconds: 0
            # Frequency of the probe.
            periodSeconds: 60
            # Number of seconds after which the probe times out.
            timeoutSeconds: 30
            # Number of times the probe is allowed to fail before the transition
            # from healthy to failure state.
            #
            # If periodSeconds = 60, 5 tries will result in five minutes of
            # checks. The proxy starts to refresh a certificate five minutes
            # before its expiration. If those five minutes lapse without a
            # successful refresh, the liveness probe will fail and the pod will be
            # restarted.
            failureThreshold: 5
