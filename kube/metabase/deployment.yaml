---
apiVersion: networking.gke.io/v1
kind: ManagedCertificate
metadata:
  name: metabase-cert
  namespace: "${NAMESPACE}"
spec:
  domains:
    - "metabase.${EAVE_ROOT_DOMAIN}"

---
apiVersion: networking.gke.io/v1beta1
kind: FrontendConfig
metadata:
  name: metabase-fc
  namespace: "${NAMESPACE}"
spec:
  redirectToHttps:
    enabled: true
    responseCodeName: MOVED_PERMANENTLY_DEFAULT

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: metabase-ingress
  namespace: "${NAMESPACE}"
  annotations:
    networking.gke.io/v1beta1.FrontendConfig: metabase-fc
    networking.gke.io/managed-certificates: metabase-cert
    kubernetes.io/ingress.global-static-ip-name: metabase-dot-eave-dot-dev-addr
    kubernetes.io/ingress.class: gce
  labels:
    app: metabase-app
spec:
  # ingressClassName: gce # This does not work. Without the "ingress.class" annotation, the LB isn't created.
  defaultBackend:
    service:
      name: metabase
      port:
        name: http

---
apiVersion: cloud.google.com/v1
kind: BackendConfig
metadata:
  name: metabase-bc
spec:
  healthCheck:
    type: HTTP
    requestPath: /api/health
    port: 3000
    checkIntervalSec: 30
    unhealthyThreshold: 4
  customResponseHeaders:
    headers:
      - "server: unspecified"

---
apiVersion: v1
kind: Service
metadata:
  name: metabase
  namespace: "${NAMESPACE}"
  annotations:
    beta.cloud.google.com/backend-config: '{"default": "metabase-bc"}'
spec:
  type: NodePort
  selector:
    app: metabase-app
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: app

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ksa-app-metabase
  namespace: "${NAMESPACE}"
  annotations:
    iam.gke.io/gcp-service-account: "gsa-app-metabase@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metabase-deployment
  namespace: "${NAMESPACE}"
  labels:
    app: metabase-app
spec:
  selector:
    matchLabels:
      app: metabase-app
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
  template:
    metadata:
      labels:
        app: metabase-app
    spec:
      serviceAccountName: ksa-app-metabase
      dnsPolicy: ClusterFirst
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      # volumes:
      #   - name: cloudsql-socket
      #     emptyDir: {}
      containers:
        - name: metabase-enterprise
          image: "metabase/metabase-enterprise:${APP_VERSION}"
          # volumeMounts:
          #   - name: cloudsql-socket
          #     mountPath: /cloudsql
          env:
            - name: MB_DB_USER
              value: "gsa-app-metabase@${GOOGLE_CLOUD_PROJECT}.iam"
            - name: MB_DB_TYPE
              value: postgres
            - name: MB_DB_HOST
              value: "127.0.0.1"
            - name: MB_DB_PORT
              value: "5432"
            - name: MB_JETTY_HOST
              value: "0.0.0.0" # Default for Docker
            - name: MB_JETTY_PORT
              value: "3000" # Default
          envFrom:
            - secretRef:
                name: metabase-secret
            - secretRef:
                name: metabase-jwt-shared-secret
            - configMapRef:
                name: metabase-configmap

          ports:
            - name: app
              containerPort: 3000

          resources:
            # requests and limits must always be the same on Autopilot clusters without bursting.
            # if requests is omitted, the limits values are used.
            limits:
              cpu: 500m
              memory: 2Gi

          securityContext:
            capabilities:
              drop:
              - NET_RAW

          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File

          # readinessProbe:
          #   httpGet:
          #     path: /api/health
          #     port: app

          livenessProbe:
            httpGet:
              path: /api/health
              port: app
            periodSeconds: 60
            timeoutSeconds: 30
            failureThreshold: 5

          startupProbe:
            httpGet:
              path: /api/health
              port: app
            periodSeconds: 10
            failureThreshold: 30

        - name: cloud-sql-proxy
          # This uses the latest version of the Cloud SQL Proxy
          # It is recommended to use a specific version for production environments.
          # See: https://github.com/GoogleCloudPlatform/cloudsql-proxy
          image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:latest
          args:
            # Enable healthcheck endpoints for kube probes
            - "--health-check"
            - "--http-address=0.0.0.0" # Bind to all interfaces so that the Kubernetes control plane can communicate with this process.
            - "--http-port=9090" # This is the default

            # If connecting from a VPC-native GKE cluster, you can use the
            # following flag to have the proxy connect over private IP
            - "--private-ip"

            # If you are not connecting with Automatic IAM, you can delete
            # the following flag.
            - "--auto-iam-authn"

            # tcp should be set to the port the proxy should listen on
            # and should match the DB_PORT value set above.
            # Defaults: MySQL: 3306, Postgres: 5432, SQLServer: 1433
            - "--port=5432"
            - "--structured-logs"
            # - "--unix-socket /cloudsql"
            - "${PG_CONNECTION_STRING}"
          # volumeMounts:
          #   - name: cloudsql-socket
          #     mountPath: /cloudsql
          securityContext:
            # The default Cloud SQL proxy image runs as the
            # "nonroot" user and group (uid: 65532) by default.
            runAsNonRoot: true

          resources:
            limits:
              cpu: 500m
              memory: 2Gi

          ports:
            - name: healthcheck
              containerPort: 9090
            - name: proxy
              containerPort: 5432

          # https://github.com/GoogleCloudPlatform/cloud-sql-proxy/tree/54e65d14a5d533f44e33b52a2dc88c2a419eae2f/examples/k8s-health-check#running-cloud-sql-proxy-with-health-checks-in-kubernetes

          startupProbe:
            httpGet:
              path: /startup
              port: healthcheck
            periodSeconds: 1
            timeoutSeconds: 5
            failureThreshold: 20

          # The documentation does not recommend using the readiness probe.
          # The cloud-sql-proxy readiness probe checks for issues that can usually resolve themselves, so this check could restart the container unnecessarily.
          # readinessProbe:
          #   httpGet:
          #     path: /readiness
          #     port: healthcheck
            # initialDelaySeconds: 30
            # # 30 sec period x 6 failures = 3 min until the pod is terminated
            # periodSeconds: 30
            # failureThreshold: 6
            # timeoutSeconds: 10
            # successThreshold: 1

          livenessProbe:
            httpGet:
              path: /liveness
              port: healthcheck
            initialDelaySeconds: 0
            periodSeconds: 60
            timeoutSeconds: 30
            # If periodSeconds = 60, 5 tries will result in five minutes of
            # checks. The proxy starts to refresh a certificate five minutes
            # before its expiration. If those five minutes lapse without a
            # successful refresh, the liveness probe will fail and the pod will be
            # restarted.
            failureThreshold: 5
